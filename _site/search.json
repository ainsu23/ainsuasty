[
  {
    "objectID": "rworld.html",
    "href": "rworld.html",
    "title": "Data Lab",
    "section": "",
    "text": "Blog\n\n\nR, Python, SQL, Spark, among others\n\n\nGeneral tips for R, Git, SQL or python.\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science\n\n\nWith tidymodels\n\n\nMachine learning models developed with tidymodels\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperations Research\n\n\nampl, gurobi, …\n\n\nApplication of OR problems\n\n\n\n\n\n\n\n\n\n\n\n\n\nShiny\n\n\nApps\n\n\nDashboards created with Shiny library for R\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dataLab/Shiny.html",
    "href": "dataLab/Shiny.html",
    "title": "Shiny",
    "section": "",
    "text": "Data Structures\n\n\nApplication for Developing Data Structures\n\n\n\nR6Class\n\n\nData Structures\n\n\n\nApplication for Developing Data Structures\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Polish\n\n\nApp with interactive games.\n\n\n\nfirebase\n\n\nAPI\n\n\n\nDiscover an interactive and enjoyable way to learn Polish with this application, featuring specially developed games designed to enhance your learning experience. Explore…\n\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Shiny\n\n\nBy Hadley Wickham\n\n\n\nmodules\n\n\n\nSolution to exercices mastering Shiny’s book.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dataLab/Shiny/learning_polish.html",
    "href": "dataLab/Shiny/learning_polish.html",
    "title": "Learning Polish",
    "section": "",
    "text": "Application Modules:\nVocabulary: This module integrates with Google sheet to store data, allowing you to add, delete, or modify words in real-time using R functions. Access dynamic and adaptable learning tailored to your needs.\nGames: The application includes two games designed to reinforce your learning:\nCategory List: Drag and drop words into their corresponding categories. Ideal for learning vocabulary classification. Guess the Word: A game where you arrange words to form correct sentences. Improve your grammar and language comprehension.\nClick on Go to App to start learning today and visit Check Code Here to explore how the application is built."
  },
  {
    "objectID": "dataLab/OperationsResearch.html",
    "href": "dataLab/OperationsResearch.html",
    "title": "Operations Research",
    "section": "",
    "text": "AMPL R API\n\n\n\nPrescriptive Analytics\n\n\nAMPL\n\n\nMIP\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dataLab/DataScience.html",
    "href": "dataLab/DataScience.html",
    "title": "Data Science",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "dataLab/Blog.html",
    "href": "dataLab/Blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDescription\n\n\nCategories\n\n\nReading Time\n\n\n\n\n\n\nData Structures with R6Class\n\n\nImplement data structures with R6Class\n\n\nR6Class, Data Structures\n\n\n4 min\n\n\n\n\nHow to interact with firebase from a shinyapp\n\n\nFind how to use httr to access or modify stored data in firebase.\n\n\nFirebase\n\n\n5 min\n\n\n\n\nPowerBI Leverage with R\n\n\nLeverage PowerBI apps with R\n\n\nPowerBI, R, functions\n\n\n3 min\n\n\n\n\nUso de Spark desde R\n\n\nBig data con Spark y R\n\n\nSpark, R6Class, Text mining\n\n\n4 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dataLab/Blog/PowerBI_R.html",
    "href": "dataLab/Blog/PowerBI_R.html",
    "title": "PowerBI Leverage with R",
    "section": "",
    "text": "PowerBI it’s a completed tool for creating dashboard, nevertheless, you can make it so much more completed leveraging with others languages, such as, R or Python."
  },
  {
    "objectID": "dataLab/Blog/PowerBI_R.html#ryan-e-wade-conference",
    "href": "dataLab/Blog/PowerBI_R.html#ryan-e-wade-conference",
    "title": "PowerBI Leverage with R",
    "section": "Ryan E Wade conference",
    "text": "Ryan E Wade conference\nWhen at my work I was assigned to co-created a dashboard in powerBI, I inmediately remember the confrence from Ryan E Wade about levering powerBI with R."
  },
  {
    "objectID": "dataLab/Blog/PowerBI_R.html#comparing-sales-within-months---dashboard",
    "href": "dataLab/Blog/PowerBI_R.html#comparing-sales-within-months---dashboard",
    "title": "PowerBI Leverage with R",
    "section": "Comparing sales within months - dashboard",
    "text": "Comparing sales within months - dashboard\nThis blog contains a very simple dashboard with just one table, my purpose indeed, it is just to show how with R we can create as many columns with hexcode colors to make change a color of a column automatically, making the dashboard reproducible in the time.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nTable Sales\n A sales table was created with just seller_id, month and the sales made.\nAfter that, two columns were created to store sales_avg and delays_avg.\n\n\nCode\nSales &lt;- tibble::tribble(\n  ~seller_id, ~month, ~sales, ~delays_percentage,\n  1, 2, 200,0.2,\n  1, 1, 400,0.04,\n  1, 3, 140,0.29,\n  1, 4, 390,0.11,\n  1, 5, 260,0.34,\n  1, 6, 130,0.23,\n  2, 1, 300,0.1,\n  2, 2, 317,0.07,\n  2, 3, 263,0.13,\n  2, 4, 142,0.21,\n  2, 5, 361,0.03,\n  2, 6, 134,0.16,\n  3, 1, 124,0.25,\n  3, 2, 374,0.23,\n  3, 3, 762,0.2,\n  3, 4, 163,0.27,\n  3, 5, 186,0.12,\n  3, 6, 177,0.09,\n) %&gt;% \n  as.data.frame() %&gt;% \n  mutate(\n      sales_avg = round(mean(sales), 1),\n      delays_avg = round(mean(delays_percentage), 2)        \n  ) \n\nSales\n\n\n   seller_id month sales delays_percentage sales_avg delays_avg\n1          1     2   200              0.20     267.9       0.17\n2          1     1   400              0.04     267.9       0.17\n3          1     3   140              0.29     267.9       0.17\n4          1     4   390              0.11     267.9       0.17\n5          1     5   260              0.34     267.9       0.17\n6          1     6   130              0.23     267.9       0.17\n7          2     1   300              0.10     267.9       0.17\n8          2     2   317              0.07     267.9       0.17\n9          2     3   263              0.13     267.9       0.17\n10         2     4   142              0.21     267.9       0.17\n11         2     5   361              0.03     267.9       0.17\n12         2     6   134              0.16     267.9       0.17\n13         3     1   124              0.25     267.9       0.17\n14         3     2   374              0.23     267.9       0.17\n15         3     3   762              0.20     267.9       0.17\n16         3     4   163              0.27     267.9       0.17\n17         3     5   186              0.12     267.9       0.17\n18         3     6   177              0.09     267.9       0.17\n\n\n\n\nCreation of new columns\n\n\n\n\n\nThe principal idea of the following function called compare it’s to generate the number of columns that the final user wants to compare and generate de hexcode column to apply functional conditions in PowerBI.\n\nfunctionFinal table\n\n\nThe compare function get a table, 2 columns to compare and the function to apply. Returns a dataframe with a hexcode color column.\n\ncompare &lt;- function(.base, .column1, .column2, .f) {\n  column &lt;- paste0(\"color_\",{{ .column1}})\n  funcion &lt;- .Primitive({{ .f }})\n  maximo &lt;- .f == \"max\"\n\n base2 &lt;- .base %&gt;%\n    mutate(\n      {{ column }} := ifelse(\n            !!rlang::sym(.column1) &gt;= funcion(!!rlang::sym(.column2), na.rm = TRUE),  \n            ifelse(maximo, \"#FF0000\", \"#00FF00\"),\n            ifelse(!maximo, \"#00FF00\",\"#FF0000\")\n      )\n    ) %&gt;%\n    ungroup() %&gt;%\n    select({{ column }})\n  return(base2)\n}\n\n\n\nStore the columns you want to compare in column1 and column2, also, write the function you wants to apply (max or min).\n\ncolumn1 &lt;- list(\"sales\", \"delays_percentage\")\ncolumn2 &lt;- list(\"sales_avg\", \"delays_avg\")\n.f&lt;- list(\"max\", \"min\")\n\n# Iterate through list of lists with pmap\ntabla &lt;- purrr::pmap(\n  .l = list(column1, column2, .f),\n  .f = function(.x, .y, .z){\n    Sales %&gt;%\n        compare(.x, .y, .z)\n  }\n) %&gt;%\n  # Convert list into a dataframe\n  purrr::flatten_df() %&gt;%\n  # column bind base with new columns flatten\n  cbind(Sales, .)\n\ntabla\n\n   seller_id month sales delays_percentage sales_avg delays_avg color_sales\n1          1     2   200              0.20     267.9       0.17     #FF0000\n2          1     1   400              0.04     267.9       0.17     #FF0000\n3          1     3   140              0.29     267.9       0.17     #FF0000\n4          1     4   390              0.11     267.9       0.17     #FF0000\n5          1     5   260              0.34     267.9       0.17     #FF0000\n6          1     6   130              0.23     267.9       0.17     #FF0000\n7          2     1   300              0.10     267.9       0.17     #FF0000\n8          2     2   317              0.07     267.9       0.17     #FF0000\n9          2     3   263              0.13     267.9       0.17     #FF0000\n10         2     4   142              0.21     267.9       0.17     #FF0000\n11         2     5   361              0.03     267.9       0.17     #FF0000\n12         2     6   134              0.16     267.9       0.17     #FF0000\n13         3     1   124              0.25     267.9       0.17     #FF0000\n14         3     2   374              0.23     267.9       0.17     #FF0000\n15         3     3   762              0.20     267.9       0.17     #FF0000\n16         3     4   163              0.27     267.9       0.17     #FF0000\n17         3     5   186              0.12     267.9       0.17     #FF0000\n18         3     6   177              0.09     267.9       0.17     #FF0000\n   color_delays_percentage\n1                  #00FF00\n2                  #00FF00\n3                  #00FF00\n4                  #00FF00\n5                  #00FF00\n6                  #00FF00\n7                  #00FF00\n8                  #00FF00\n9                  #00FF00\n10                 #00FF00\n11                 #00FF00\n12                 #00FF00\n13                 #00FF00\n14                 #00FF00\n15                 #00FF00\n16                 #00FF00\n17                 #00FF00\n18                 #00FF00"
  },
  {
    "objectID": "dataLab/Blog/PowerBI_R.html#final-result-in-powerbi",
    "href": "dataLab/Blog/PowerBI_R.html#final-result-in-powerbi",
    "title": "PowerBI Leverage with R",
    "section": "Final result in PowerBI",
    "text": "Final result in PowerBI\nWith the following result I invite you to integrate R scripts with PowerBI so you can create powerfull apps."
  },
  {
    "objectID": "dataLab/Blog/data_structures.html",
    "href": "dataLab/Blog/data_structures.html",
    "title": "Data Structures with R6Class",
    "section": "",
    "text": "Data Structures\nThis post is oriented to create classes that recreates data structures and explanaition for each. Knowing this may help you improve as a programmer because is the basic of each language you would work R, python, c, javascript, others..\nData structures to work with:\n\nArrays\nLinkedLists\nHashtable\nStacks\nQueues\nTrees\nGraphs\n\n\nArraysLinkedLists\n\n\nR manages dinamics vectors, this means that one index can be added to an existed vector. For the purpose of this blog I would create a class R6Class to simmulate an Array. Within this array, you would be able to get an index, push a new item at the end, pop last item, delete an index.\n\nmyarray &lt;- R6::R6Class(\n  classname = \"myarray\",\n  public = list(\n    initialize = function() {\n      self$array_length &lt;- 0\n      self$array_data &lt;- c()\n    },\n    array_length = NULL,\n    array_data = NULL,\n    get_value = function(index) {\n      return(self$array_data[index])\n    },\n    push = function(value) {\n      self$array_data[self$array_length + 1] = value\n      self$array_length &lt;- length(self$array_data)\n    },\n    pop = function() {\n      lastItem &lt;- self$array_data[self$array_length]\n      self$array_data &lt;- self$array_data[-self$array_length]\n      self$array_length &lt;- length(self$array_data)\n      return(lastItem)\n    },\n    delete = function(index) {\n      self$array_data &lt;- self$array_data[-index]\n      self$array_length &lt;- length(self$array_data)\n    }\n  )\n)\nmyarray = myarray$new()\nmyarray$push(2)\nmyarray$pop()\n\n\n\nLinkedLists are a set of nodes (that contains information related to where the data is stored in pc memory (pointers) and which node is next or previous). Until my understanding, R does not have linkedList in its implementation, list are manage as vectors or arrays. There exists 2 types or linked lists, one is single which it has just one direction and double which has two directions.\nNevertheless, let’s implement an double linkedList with R6Class:\n\nmy_Linked_List &lt;- R6::R6Class(\n  classname = \"linkedList\",\n  public = list(\n    # Initialize with the first value of the linkedList, .next would be NULL\n    initialize = function(value) {\n      self$list_pointer &lt;- list(new.env())\n      self$list_pointer[[1]]$value &lt;- value\n      self$index_head &lt;- 1\n      self$index_tail &lt;- 1\n      self$list_pointer[[1]]$index_next &lt;- NULL\n      self$list_pointer[[1]]$index_prev &lt;- NULL\n    },\n    list_pointer = NULL,\n    index_head = NULL,\n    index_tail = NULL,\n    # next is an used variable from R.\n    get_index = function(index) {\n      if (self$index_tail &lt; index) stop(\"index not created yet!\")\n      return(self$list_pointer[[index]]$value)\n    },\n    insert = function(value) {\n      self$list_pointer &lt;- self$list_pointer %&gt;%\n        append(new.env())\n      self$index_tail &lt;- self$index_tail + 1\n      self$list_pointer[[self$index_tail]]$index_prev &lt;-\n        self$list_pointer[[self$index_tail - 1]]\n      self$list_pointer[[self$index_tail - 1]]$index_next &lt;-\n        self$list_pointer[[self$index_tail]]\n      self$list_pointer[[self$index_tail]]$value &lt;- value\n      self$list_pointer[[self$index_tail]]$index_next &lt;- NULL\n      return(self$list_pointer)\n    }\n  )\n)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataLab/Blog/interact_with_firebase.html",
    "href": "dataLab/Blog/interact_with_firebase.html",
    "title": "How to interact with firebase from a shinyapp",
    "section": "",
    "text": "At the planning phase of creating a shiny app you would find the importance of having storing data and interact with the stored system.\nYou might have interaction with data bases using dbplyr, DBI, among others packages. Reading this blog you will find how to use httr in order to access or modify stored data in firebase."
  },
  {
    "objectID": "dataLab/Blog/interact_with_firebase.html#interacting-with-firebase-from-r",
    "href": "dataLab/Blog/interact_with_firebase.html#interacting-with-firebase-from-r",
    "title": "How to interact with firebase from a shinyapp",
    "section": "Interacting with firebase from R",
    "text": "Interacting with firebase from R\nFirstly, you need to have all setup in firebase so R can connect trough API, it is recommended to store the API_KEY, firebase_url and password in the .Renviron file in the root of the app (where ui.R and server.R are stored or app.R).\nSecondly, it is very important to be familiar with JSON structures in order to design how you are going to store your data in firebase.\nFrom previous, you might want to bring the information, update, delete, insert, among others. Let’s build together the select.\n\nDefine JSON structure to store data\nIn my learning polish app, I design to have a list of words, this list would contains a list of categories, and each of this would have the register with the word, translation in spanish and date of insertion in a simple text.\nFor the purpose of the blog I am going to use a fragment of data from my learning polish shiny app.\n\n{\n  \"words\" : {\n    \"animals\" : [\n        \"pies: perro: 2022-01-23\",\n        \"kot: gato: 2022-01-23\",\n        \"biedronka: mariquita: 2022-01-23\",\n        \"Ptak: Pájaro: 2022-01-23\",\n        \"Komar: Mosquito: 2022-01-23\", ],\n    \"clothes\" : [\n        \"buty: zapatos:2022-01-26\",\n        \"spodnie: pantalón: 2022-01-30\",\n        \"sweter: sueter: 2022-02-22\",\n        \"krawat: corbata: 2022-02-22\",\n        \"koszula: camisa: 2022-02-22\" ],\n  }\n}\n\n\n\nSelecting data from firebase\nThe firebase url given by google is the place where your data is stored. It will look somethis as followwing: “https://name-hash_given_firebase-default-rtdb.firebaseio.com/”\nIf you would like to access to the words inside the category clothes, you might add the list words and clothes in the previous link, as follors:\n“https://name-hash_given_firebase-default-rtdb.firebaseio.com/words/clothes”\nIn the documentary from firebase, you can find that you need to add .json when you are using an API (I invite you to read documentation to more detail).\n\nselect_words &lt;- function(categories) {\n    words &lt;- httr::content(\n      httr::GET(\n        paste0(\n          Sys.getenv(\"FIREBASE_URL\"), \"/words/\", categories, \".json\")\n      )\n    ) %&gt;%\n    purrr::flatten() %&gt;%\n    unlist()\n  return(words)\n}\n\n\nselect_categories &lt;- function() {\n  categories &lt;- content(GET(\n    paste0(Sys.getenv(\"FIREBASE_URL\"), \"/words/.json\")\n  ))\n  return(categories)\n}\n\n\n\nInsert data to firebase:\nFor inserting data you can use the function PUT from httr. Into words variable we bring the words from category so we can add the new word to already stored words and converted to json with jsonlite package.\n\nadd_words &lt;- function(categories, word) {\n  if (word != \"\") {\n    words &lt;- select_words(categories)\n    body &lt;- jsonlite::toJSON(c(words, word),\n      pretty = TRUE\n    )\n    response &lt;- httr::PUT(\n      paste0(Sys.getenv(\"FIREBASE_URL\"), \"/words/\", categories, \".json\"),\n      body = body\n    )\n  }\n}\n\n\n\nDelete data stored in firebase from R.\nThe following function receives the name of the category and one or more words (example: ptak and komar).\nThe first purrr::map compares each word with the list of words inside the category and save the position where it is stored in firebase.\nThe second purrr::map iterates over positions and tells firebase wich position to DELETE.\n\ndelete_words &lt;- function(categories, word) {\n  words_delete &lt;- purrr::map(\n    .x = stringr::str_to_lower(word),\n    .f = function(.x) {\n      content(GET(\n        paste0(Sys.getenv(\"FIREBASE_URL\"), \"/words/\", categories, \".json\")\n      )) %&gt;%\n        stringi::stri_trans_tolower(.) %&gt;%\n        unique() %&gt;%\n        stringr::str_starts(.x) %&gt;%\n        which() - 1\n    }\n  )\n  purrr::map(\n    .x = words_delete,\n    .f = function(.x) {\n      httr::DELETE(\n        paste0(\n          Sys.getenv(\"FIREBASE_URL\"), \"words/\", categories, \"/\", .x, \".json\"\n        )\n      )\n    }\n  )\n}\n\nThanks for reading. Any comments or feedback I would love to hear from you, you can have my info from contact."
  },
  {
    "objectID": "dataLab/Blog/spark_desde_R.html",
    "href": "dataLab/Blog/spark_desde_R.html",
    "title": "Uso de Spark desde R",
    "section": "",
    "text": "El uso de Spark en DataScience es ahora más común por sus grandes beneficios. Algunos son: almacenamiento distribuido, uso de queries como si se estuviera escribiendo en SQL, desarrollo de modelos de machine learning, entre muchos otros.\nRstudio (Posit) ha desarrollado el paquete sparklyr, el cual me parece que es muy completo. Súper recomendado!!\nLos ejercicios de este blog provienen del curso de Udemy: “Taming Big Data with Apache Spark and Python - Hands On!” sin embargo se resolvieron con Sparklyr y la conexión se realiza de forma local."
  },
  {
    "objectID": "dataLab/Blog/spark_desde_R.html#cargue-librerías",
    "href": "dataLab/Blog/spark_desde_R.html#cargue-librerías",
    "title": "Uso de Spark desde R",
    "section": "Cargue librerías",
    "text": "Cargue librerías\n\nlibrary(R6)\nlibrary(tidyverse)\nlibrary(sparklyr)"
  },
  {
    "objectID": "dataLab/Blog/spark_desde_R.html#clase-r6-spark_conexion.",
    "href": "dataLab/Blog/spark_desde_R.html#clase-r6-spark_conexion.",
    "title": "Uso de Spark desde R",
    "section": "Clase R6 spark_conexion.",
    "text": "Clase R6 spark_conexion.\nLa clase spark_conexion mantiene toda información y métodos relevantes de la conexión a Spark. Permitiendo reutilizarlas en todo el código.\n\nspark_conexion &lt;- R6::R6Class(\n  classname = \"conexion\",\n  public = list(\n    initialize = function() {\n      return(invisible(self))\n    },\n    #' @description\n    #' Crea nuevo objeto de conexión\n    #' @details\n    #' Esta función requiere unas variables de ambiente cargadas para poder\n    #' functionar.\n    connect = function() {\n     self$conn_sp &lt;- sparklyr::spark_connect(master = \"local\")\n      return(invisible(self))\n    },\n    #' @field conn_spark conexion a spack\n    conn_sp  = NULL,\n    #' @descripcion Método para dplyr::copy_to con conexion a spark\n    #' @param tabla_df Data frame\n    #' @param tbl_name Nombre de la tabla en spark\n    copy_to_sp = function(tabla_df, tbl_name) {\n      copy_to(self$conn_sp, tabla_df, tbl_name) \n    },\n    #' @descripcion Método para leer tabla de spark\n    #' @param tbl_name Nombre de la tabla en spark\n    tbl_sp = function(tbl_name) spark_read_table(self$conn_sp, tbl_name)\n  )\n)\n\nconn &lt;- spark_conexion$new()$connect()"
  },
  {
    "objectID": "dataLab/Blog/spark_desde_R.html#operaciones-básicas-con-dplyr",
    "href": "dataLab/Blog/spark_desde_R.html#operaciones-básicas-con-dplyr",
    "title": "Uso de Spark desde R",
    "section": "Operaciones básicas con dplyr",
    "text": "Operaciones básicas con dplyr\nArchivo extraído de grouplens.org.\nVentajas de usar spark desde R:\n\nSe pueden usar los verbos de dplyr\nLazy evaluation: Ver que en la parte superior del resultado aparece: # Source: spark&lt;?&gt; [?? x 2])\n\n\ngroup_by y countsummarise, arrangefilter min\n\n\n\nsparklyr::spark_read_text(\n  conn$conn_sp, \n  name = \"movieLens\",\n  \"../../data/u.data\"\n) %&gt;% \n  separate(line, c(\"user id\",\"item id\",\"rating\",\"timestamp\"), sep = \"\\t\") %&gt;% \n  dplyr::group_by(rating) %&gt;% \n  dplyr::count()\n\n# Source:   SQL [5 x 2]\n# Database: spark_connection\n# Groups:   rating\n  rating     n\n  &lt;chr&gt;  &lt;dbl&gt;\n1 3      27145\n2 1       6110\n3 2      11370\n4 4      34174\n5 5      21201\n\n\n\n\nTambién permite usar summarise, arrange y operaciones dentro de summarise cómo round, mean.\n\nsp_fake_friends &lt;- sparklyr::spark_read_csv(\n  conn$conn_sp,\n  name = \"fakefriends\",\n  \"../../data/fakefriends.csv\",\n  header = FALSE,\n  columns = c(\"id\", \"name\", \"age\", \"num_friends\")\n) %&gt;% \n  dplyr::group_by(age) %&gt;% \n  dplyr::summarise(num_friends = round(mean(num_friends), 1)) %&gt;% \n  dplyr::arrange(age)\nsp_fake_friends\n\n# Source:     SQL [?? x 2]\n# Database:   spark_connection\n# Ordered by: age\n   age   num_friends\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 18           343.\n 2 19           213.\n 3 20           165 \n 4 21           351.\n 5 22           206.\n 6 23           246.\n 7 24           234.\n 8 25           198.\n 9 26           242.\n10 27           228.\n# ℹ more rows\n\n\n\n\nLa función dplyr::filter puede entrar en conflicto con la funcion sparklyr::filter\n\nsparklyr::spark_read_csv(\n  conn$conn_sp,\n  name = \"fakefriends\",\n  \"../../data/fakefriends.csv\",\n  header = FALSE,\n  columns = c(\"id\", \"name\", \"age\", \"num_friends\")\n) %&gt;% \n  dplyr::filter(age == min(age))\n\n# Source:   SQL [8 x 4]\n# Database: spark_connection\n  id    name    age   num_friends\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;      \n1 106   Beverly 18    499        \n2 115   Dukat   18    397        \n3 341   Data    18    326        \n4 377   Beverly 18    418        \n5 404   Kasidy  18    24         \n6 439   Data    18    417        \n7 444   Keiko   18    472        \n8 494   Kasidy  18    194"
  },
  {
    "objectID": "dataLab/Blog/spark_desde_R.html#operaciones-con-texto",
    "href": "dataLab/Blog/spark_desde_R.html#operaciones-con-texto",
    "title": "Uso de Spark desde R",
    "section": "Operaciones con texto",
    "text": "Operaciones con texto\nLas operaciones con texto también pueden ser usadas mediante verbos o secuencia tidyverse.\n\nft_tokenizer: Esta función permite almacenar las palabras de la fila en una lista.\n\nft_stop_words_remover: Se eliminan las palabras conexión tales como: a, en,   entre, o, aquí, aún, con, de, e, y, hay, ...\n\n\nsparklyr::spark_read_text(\n  conn$conn_sp,\n  path = \"../../data/Book\"\n) %&gt;% \n  ft_tokenizer(\n    input_col = \"line\",\n    output_col = \"word_list\"\n      \n  ) %&gt;% \n  ft_stop_words_remover(\n    input_col = \"word_list\",\n    output_col = \"wo_stop_words\"\n  ) %&gt;% \n  dplyr::mutate(palabra = explode(wo_stop_words)) %&gt;% \n  dplyr::filter(palabra != \"\") %&gt;% \n  dplyr::group_by(palabra) %&gt;% \n  dplyr::count() %&gt;% \n  dplyr::filter(palabra != \"�\") %&gt;% \n  dplyr::arrange(desc(n)) %&gt;% \n  head(10)\n\n# Source:     SQL [10 x 2]\n# Database:   spark_connection\n# Groups:     palabra\n# Ordered by: desc(n)\n   palabra      n\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 business   290\n 2 time       168\n 3 need       167\n 4 new        150\n 5 product    128\n 6 people     127\n 7 get        122\n 8 work       120\n 9 may        107\n10 want       107"
  },
  {
    "objectID": "dataLab/Blog/spark_desde_R.html#machine-learning",
    "href": "dataLab/Blog/spark_desde_R.html#machine-learning",
    "title": "Uso de Spark desde R",
    "section": "Machine Learning",
    "text": "Machine Learning\nEn mi opinión los beneficios que encontré de aplicar ML con sparklyr son:\n\nPipelines: Conjunto de pasos que se desean aplicar al modelo en construcción, es decir, las operaciones a la base, la formula del modelo, seleccion el algoritmo a desarrollar (regresión lineal, árbol de decisión).\nAlgoritmos: Sparklyr usa la librería de ML de Spark, por ende, cuenta con una gran variedad de algoritmos para ser usados.\nTransformaciones: ft_dplyr_transformer permite aplicar operaciones con dplyr y aplicarlo en el pipeline creado.\n\n\nLinear Regression\n\ndataModelo\n\n\n\n\nCódigo\nsdf_regresion &lt;- sparklyr::spark_read_text(\n  conn$conn_sp,\n  path = \"../../data/regression.txt\"\n) %&gt;% \n  separate(line, c(\"x\", \"y\"), \",\") %&gt;% \n  mutate(across(where(is.character), as.numeric))\n\nsdf_regresion\n\n\n# Source:   SQL [?? x 2]\n# Database: spark_connection\n       x     y\n   &lt;dbl&gt; &lt;dbl&gt;\n 1 -1.74  1.66\n 2  1.24 -1.18\n 3  0.29 -0.4 \n 4 -0.13  0.09\n 5 -0.39  0.38\n 6 -1.79  1.73\n 7  0.71 -0.77\n 8  1.39 -1.48\n 9  1.15 -1.43\n10  0.13 -0.07\n# ℹ more rows\n\n\n\n\n\nregresion_pipeline &lt;- sparklyr::ml_pipeline(conn$conn_sp) %&gt;%\n  sparklyr::ft_r_formula(y ~ x) %&gt;%\n  sparklyr::ml_linear_regression()\n\npartitioned_regresion &lt;- sparklyr::sdf_random_split(\n  sdf_regresion,\n  training = 0.7,\n  testing = 0.3\n)\n\nfitted_pipeline &lt;- sparklyr::ml_fit(\n  regresion_pipeline,\n  partitioned_regresion$training\n)\n\npredictions &lt;- sparklyr::ml_transform(\n  fitted_pipeline,\n  partitioned_regresion$testing\n)\n\npredictions\n\n# Source:   table&lt;`sparklyr_tmp_c49e0ccf_7ab1_4b4c_be90_c2838d184847`&gt; [?? x 5]\n# Database: spark_connection\n       x     y features  label prediction\n   &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1 -3.74  3.75 &lt;dbl [1]&gt;  3.75       3.72\n 2 -2.89  2.89 &lt;dbl [1]&gt;  2.89       2.88\n 3 -2.58  2.57 &lt;dbl [1]&gt;  2.57       2.57\n 4 -2.45  2.44 &lt;dbl [1]&gt;  2.44       2.44\n 5 -2.36  2.43 &lt;dbl [1]&gt;  2.43       2.35\n 6 -2.29  2.35 &lt;dbl [1]&gt;  2.35       2.28\n 7 -2.27  2.19 &lt;dbl [1]&gt;  2.19       2.26\n 8 -2.06  1.95 &lt;dbl [1]&gt;  1.95       2.05\n 9 -2     2.02 &lt;dbl [1]&gt;  2.02       1.99\n10 -1.91  1.83 &lt;dbl [1]&gt;  1.83       1.90\n# ℹ more rows"
  },
  {
    "objectID": "dataLab/OperationsResearch/AMPL_R_API.html",
    "href": "dataLab/OperationsResearch/AMPL_R_API.html",
    "title": "AMPL R API",
    "section": "",
    "text": "Have you ever asked, How to optimize your commercial process with Mathematical Optimization (field: Operations Research)?\nOperations Research is an old field that have had improved many industries around the world with the use of mathematics, it helps to model a real problem with an objective function and constrains associated to it.\nIn this post, I would show how you can run an optimization problem from R using the integration with AMPL (Optimization software). To get more details about this integration, please follow this link"
  },
  {
    "objectID": "dataLab/OperationsResearch/AMPL_R_API.html#libraries",
    "href": "dataLab/OperationsResearch/AMPL_R_API.html#libraries",
    "title": "AMPL R API",
    "section": "Libraries",
    "text": "Libraries\nIn order to install rAMPL it is important to have the lastest version of RTools installed. Getting started session from AMPL webiste.\nAs long as , used renv for building my blog I use the following code:\n\nrenv::install(\"Rcpp\", type=\"source\")\nrenv::install(\"https://ampl.com/dl/API/rAMPL.tar.gz\", repos=NULL, INSTALL_opts=c(\"--no-multiarch\", \"--no-staged-install\"))\n\n\nlibrary(rAMPL)\nlibrary(dplyr)\nlibrary(DT)"
  },
  {
    "objectID": "dataLab/OperationsResearch/AMPL_R_API.html#rampl-manage-class-structure",
    "href": "dataLab/OperationsResearch/AMPL_R_API.html#rampl-manage-class-structure",
    "title": "AMPL R API",
    "section": "rAMPL manage Class structure",
    "text": "rAMPL manage Class structure\nThe idea behind this package is to manage the optimization problem as an instance of a class.\nThat means a variable with the content of the class needs to be created. This object will contains the structure of the problem, each time the user need to introduces\ninformation. For example: the optimization formulation is build in the eval method or loaded from a .mod file.\nIn case you struggle with this concept of classes, you can check the post Data Structures with R6Class\n\n# env &lt;- new(Environment, \"full path to the AMPL installation directory\")\nampl &lt;- new(AMPL, env)\nampl$setOption(\"solver\",\"HiGHS\") \nampl$eval(\"var x;\")\nampl$eval(\"maximize z: x;\")\nampl$eval(\"subject to c: x&lt;=10;\")\n\nx &lt;- ampl$getVariable(\"x\")\n\nampl$solve()\n\nHiGHS 1.7.0: \b\b\b\b\b\b\b\b\b\b\b\b\bHiGHS 1.7.0: optimal solution; objective 10\n0 simplex iterations\n0 barrier iterations\n\n#\n# # At this point x$value() evaluates to 10\nprint(x$value())  # prints 10\n\n[1] 10\n\nampl$close()"
  },
  {
    "objectID": "dataLab/OperationsResearch/AMPL_R_API.html#assign-clients-to-commercials",
    "href": "dataLab/OperationsResearch/AMPL_R_API.html#assign-clients-to-commercials",
    "title": "AMPL R API",
    "section": "Assign clients to commercials",
    "text": "Assign clients to commercials\nIn a previous job, I worked as the responsible of commercials campaigns and one task was to assign the clients to the commercials having into account some constrains; such as:\n\nMonthly capacity by hierarchy\nOne client had to be attended by one salesperson\nOne commercial with XX knowledge could not attend one client that would requires attention on KK.\n\nThis task was done with excel creating pivot tables and crossing them and counting manually which client was assigned and tried to reach the capacity constrain. Nevertheless, this problem is a classic problem to be solved with operations research.\n\nParameters\n\nn_clients &lt;- 84\nn_commercials &lt;- 4\nn_campaigns &lt;- 4\nn_rol &lt;- 3\n\n\n\nData\nThe data used was generated using R and stored in dataframes. AMPL can read dataframes, so it is not need of having the data vectorized, as for example, in ompr package.\n\nCommercialClientsCampaignsJoined tables\n\n\nThis table has the list of workers in sales. For each person, contains the rol, the conversion_rate (average), and capacity in the month.\n\ncommercials &lt;- data.frame(\n  \"seller_id\" = sample(1:n_commercials, replace = FALSE),\n  \"rol\" = 1:n_commercials %&gt;% \n      purrr::map(function(x){\n        sample(LETTERS[1:n_rol],1,replace = TRUE)\n      }) %&gt;% \n    unlist(),\n  \"convertion_rate\" = sample(20:100, n_commercials, replace = FALSE) / 100\n  ) %&gt;% \n  left_join(\n    data.frame(\n      \"rol\" = LETTERS[1:n_rol],\n      \"capacity\" = 1:n_rol %&gt;% \n          purrr::map(function(x){\n            sample(10:20,1,replace = FALSE)\n          }) %&gt;% \n        unlist()\n    ),\n    by = \"rol\"\n    )\ncommercials %&gt;% arrange(seller_id) %&gt;% DT::datatable()\n\n\n\n\n\n\n\nThis table has the list of clients and campaigns to be offer.\n\nclients &lt;- data.frame(\n  \"client_id\" = sample(1:n_clients,replace = FALSE),\n  \"campaign\" = 1 %&gt;% purrr::map(function(x){\n    paste(\"campaign_\",sample(1:n_campaigns, n_clients, replace = TRUE),sep = \"\")\n  }) %&gt;% unlist()\n) %&gt;% \n  left_join(\n    data.frame(\n      \"campaign\" = paste0(\"campaign_\", sample(1:n_campaigns, n_campaigns, replace = FALSE)),\n      \"benefit\" = 1:n_campaigns %&gt;% \n          purrr::map(function(x){\n            sample(100:1000, 1, replace = FALSE)\n          }) %&gt;% \n        unlist()\n    ),\n    by = \"campaign\"\n  )\nclients %&gt;% arrange(client_id) %&gt;% DT::datatable()\n\n\n\n\n\n\n\nThis table has which role can attend each campaign.\n\ncampaigns &lt;- data.frame(\n  \"campaign\" = paste(\"campaign_\",1:n_campaigns,sep = \"\"),\n  \"rol\" = 1:(n_campaigns) %&gt;% purrr::map(function(x){\n    sample(LETTERS[1:n_rol],1,replace = TRUE)\n  }) %&gt;% unlist()\n) %&gt;% distinct()\n\ncampaigns %&gt;% arrange(rol) %&gt;% DT::datatable()\n\n\n\n\n\n\n\nFinally, the previous tables are joined to check data before start modelling.\n\nfinnal &lt;- clients %&gt;% \n  left_join(campaigns, by = \"campaign\") %&gt;% \n  left_join(commercials, by = \"rol\") %&gt;% \n  mutate(benefit = benefit * convertion_rate)\n\nWarning in left_join(., commercials, by = \"rol\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 4 of `x` matches multiple rows in `y`.\nℹ Row 2 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nfinnal %&gt;% DT::datatable(filter = 'top')\n\n\n\n\n\n\n\n\n\n\nModel\nNow it is time to model, it is a good practice (even in AMPL) to have the .model, .data and .run files separated in one folder. As I build here the data, I just create the data as vectors for sets or scalar parameters or dataframes for tables\n\nModel run in RAMPL .mod\n\n\n\n## Build data to pass AMPL Model\n\n### sets\nClients &lt;- distinct(finnal, client_id)[,1]\nCommercials &lt;- distinct(finnal, seller_id)[,1]\n### paramets associated to commercial\ncapacities &lt;- distinct(finnal, seller_id, capacity)[,2]\n### parameter associated to client and commercial\nbenefit &lt;- select(finnal, client_id, seller_id, benefit) %&gt;% \n  tidyr::pivot_wider(names_from = seller_id, values_from = benefit) %&gt;% \n  mutate(across(where(is.numeric), ~tidyr::replace_na(.x, 1))) %&gt;% \n  tidyr::pivot_longer(!client_id, names_to = \"seller_id\", values_to = \"benefit\") %&gt;% \n  mutate(seller_id = as.numeric(seller_id))\n\n## .run \nampl &lt;- new(AMPL, env) # Create class ampl\n\n# Setting solver to be used. Due to limit license (max 300 vars or contrains), \n# I change to HiGHS solver, allowed with AMPL CE..\n# an open source solver.\nampl$setOption(\"solver\",\"HiGHS\") \n\n#reading model written in .mod file (AMPL)\nampl$read(\"models/assign.mod\") # Read model located in folder models\n\n# Defines sets data and parameters.\n\nampl$setData(data.frame(Clients = Clients), 1, \"Clients\") \nampl$setData(\n  data.frame(Commercials = Commercials, capacity = capacities), \n  1, \n  \"Commercials\"\n)\nampl$setData(benefit, 2, \"\")\n\nampl$solve()\n\nHiGHS 1.7.0: \b\b\b\b\b\b\b\b\b\b\b\b\bHiGHS 1.7.0: optimal solution; objective 15727.92\n10 simplex iterations\n1 branching nodes\n\n## Get objective solution\ncat(sprintf(\"Objective: %f\\n\", ampl$getObjective(\"Profit\")$value()))\n\nObjective: 15727.920000\n\n# Get the values of the variable assign in a data.frame\ndf &lt;- ampl$getVariable(\"assign\")$getValues()\n\ndf &lt;- df %&gt;% \n  rename(\n    seller_id = index1,\n    client_id = index0,\n    solution = assign.val\n  ) %&gt;% \n  mutate(solution = round(solution))\n\nampl$close()\n\n\n\nContains the formulation of the problem written in AMPL language.\n```{AMPL}\n\nset Clients;\nset Commercials;\n\nparam capacity {Commercials} &gt; 0;\nparam benefit {Clients, Commercials} &gt;= 0;\n\nvar assign {Clients, Commercials} binary;\n\nmaximize Profit:\n  sum {i in Clients, j in Commercials} benefit[i,j] * assign[i,j];\n  \nsubject to Supply {i in Clients}:\n  sum {j in Commercials} assign[i,j] &lt;= 1;\nsubject to capacity_constrain {j in Commercials}:\n  sum {i in Clients} assign[i,j] &lt;= capacity[j];\n```\n\n\n\n\nSolutionConstrain capacityConstrain assign\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome clients were not assigned due to capacity of the sales force.\n\n\n\ndf %&gt;% \n  DT::datatable(filter = 'top')\n\n\n\n\n\n\n\n\ndf %&gt;% \n  group_by(seller_id) %&gt;% \n  summarise(num_clients = sum(solution)) %&gt;% \n  DT::datatable(filter = 'top')\n\n\n\n\n\n\n\n\ndf %&gt;% \n  group_by(client_id) %&gt;% \n  summarise(num_commercials = sum(solution)) %&gt;% \n  DT::datatable(filter = 'top')\n\n\n\n\n\n\n\n\nThanks for reading, Hope this would be helpfull for you or your organization."
  },
  {
    "objectID": "dataLab/Shiny/data_structures.html",
    "href": "dataLab/Shiny/data_structures.html",
    "title": "Data Structures",
    "section": "",
    "text": "Go to App\nCheck code here\n\nThis application is designed to both learn and create applications based on the field of data structures.\nIt features a double-linked list representation with random words. Users must guess the word from a word cloud. The letters entered are compared to a linked list stored in a R6Class."
  },
  {
    "objectID": "dataLab/Shiny/mastering_shiny.html",
    "href": "dataLab/Shiny/mastering_shiny.html",
    "title": "Mastering Shiny",
    "section": "",
    "text": "This application solves exercises proposed on each chapter of the book.\n\nGo to App\nCheck code here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrés Felipe Insuasty Ch.",
    "section": "",
    "text": "Hola | Hi | Cześć\nI love learn and share what I have learned, for that reason, I share with you personal experiences and profesional projects developing Data Science."
  }
]